{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7318fa10-9bce-492b-8f45-414950cf5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the library Setup\n",
    "from equity_downloader import *\n",
    "from datetime import date, timedelta, datetime\n",
    "import random\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from itertools import chain\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad26ff-f3fe-4ca5-9230-b85d753d13b8",
   "metadata": {},
   "source": [
    "### Import the index and base equity file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d831a9d6-d85a-4537-925f-90a4ce0197b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_base = pd.read_csv(\"Misc Files/index_base.csv\", low_memory=False)\n",
    "index_base['Index Date'] = [convert_date(x, format= \"%d-%m-%Y\") for x in index_base['Index Date'].tolist()]\n",
    "index_base['DATE'] = pd.to_datetime(index_base['Index Date'], format='%Y-%m-%d')\n",
    "index_grp = index_base.groupby('Index Name')['DATE'].count().reset_index()\n",
    "index_grp = index_grp[index_grp['DATE']>800]\n",
    "index_base = index_base[index_base['Index Name'].isin(index_grp['Index Name'].tolist())]\n",
    "index_base.drop_duplicates(['Index Name', 'DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf59d3fb-f854-4530-84ca-e95c05f3cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_df_industry = pd.read_csv(\"Misc Files/bhavcopy_modeldf.csv\", low_memory=False)\n",
    "eq_df_industry['DATE'] = pd.to_datetime(eq_df_industry['DATE'], format='%Y-%m-%d')\n",
    "eq_df_industry.drop_duplicates(['SYMBOL', 'DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3809cc76-57dc-4927-aa21-bc2fb07c0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "SYMBOL_COLS = eq_df_industry['SYMBOL'].unique()\n",
    "industry_index = ['Nifty 50', 'Nifty Auto','Nifty Bank','Nifty Energy','Nifty Financial Services','Nifty FMCG'\n",
    "                  ,'Nifty IT','Nifty Media','Nifty Metal','Nifty MNC','Nifty Pharma','Nifty PSU Bank','Nifty Realty'\n",
    "                  ,'Nifty India Consumption','Nifty Commodities','Nifty Dividend Opportunities 50','Nifty Infrastructure'\n",
    "                  ,'Nifty PSE','Nifty Services Sector','Nifty India Digital','Nifty Mobility','Nifty India Defence'\n",
    "                  ,'Nifty Financial Services Ex-Bank','Nifty Housing','Nifty Transportation & Logistics'\n",
    "                  ,'Nifty MidSmall Financial Services','Nifty MidSmall Healthcare','Nifty MidSmall IT & Telecom'\n",
    "                  ,'Nifty MidSmall India Consumption','Nifty REITs & InvITs','Nifty Core Housing','Nifty Consumer Durables'\n",
    "                  ,'Nifty Non-Cyclical Consumer','Nifty India Manufacturing','Nifty Private Bank','NIFTY SME EMERGE'\n",
    "                  ,'Nifty Oil & Gas','Nifty Healthcare Index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cec4a-3243-4525-9527-0358313b61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████▉           | 526/610 [04:26<00:43,  1.92it/s]"
     ]
    }
   ],
   "source": [
    "corr_drop=False\n",
    "perf_agg_14 = pd.DataFrame(columns = ['SYMBOL', 'Best_Cutoff_14', 'Rank_Cutoff_14','Train_Accuracy_14', 'Test_Accuracy_14', 'AUC_TRAIN_14', 'AUC_test_14'])\n",
    "perf_agg_30 = pd.DataFrame(columns = ['SYMBOL', 'Best_Cutoff_30', 'Rank_Cutoff_30', 'Train_Accuracy_30', 'Test_Accuracy_30', 'AUC_TRAIN_30', 'AUC_test_30'])\n",
    "\n",
    "target_df = {'TARGET_14':perf_agg_14, 'TARGET_30':perf_agg_30}\n",
    "target_col = ['TARGET_14', 'TARGET_30']\n",
    "target_suffix = {'TARGET_14':['Prob_14', 'Predicted_14'], 'TARGET_30':['Prob_30', 'Predicted_30']}\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(SYMBOL_COLS))):\n",
    "    SYMBOL = SYMBOL_COLS[i]\n",
    "    #----------------------------------Include index features----------------------------------------------\n",
    "    eq_symbol_in = eq_data_pred(eq_df_industry = eq_df_industry, symbol=SYMBOL, keep_ma=True)\n",
    "    eq_symbol_index = pd.merge(eq_symbol_in[['DATE', 'CLOSE_PRICE']], index_base, on=[\"DATE\"])\n",
    "\n",
    "    tt = eq_symbol_index[['Index Name', 'CLOSE_PRICE', 'Closing Index Value']].groupby('Index Name').corr().reset_index()\n",
    "    tt1 = tt[tt['level_1']=='CLOSE_PRICE'][['Index Name', 'Closing Index Value']].copy()\n",
    "    tt2 = tt1[(tt1['Closing Index Value']>0.8)].copy()\n",
    "    if tt2.shape[0]>0:\n",
    "        index = tt2.sort_values(by=\"Closing Index Value\").tail(1)['Index Name'].tolist()[0]\n",
    "    else:\n",
    "        index=None\n",
    "\n",
    "    if index is not None:\n",
    "        index_select=eq_symbol_index[eq_symbol_index['Index Name']==index][['Index Name', 'DATE', 'Closing Index Value']].copy()\n",
    "        index_select.sort_values(by = \"DATE\", inplace=True)\n",
    "        # There might be few empty values where index value is missing. First get the full dates\n",
    "        index_full_date = pd.merge(eq_symbol_in[['DATE']], index_select, on=[\"DATE\"], how=\"left\")\n",
    "        index_full_date['Closing Index Value'] = index_full_date['Closing Index Value'].ffill()\n",
    "        index_full_date['GRAD_INDEX'] = np.gradient(index_full_date['Closing Index Value'], np.arange(index_full_date['Closing Index Value'].shape[0]))\n",
    "        lags = [7, 14, 30, 45, 60, 90, 180]\n",
    "        for lag in lags:\n",
    "            grad_col = f'GRAD_INDEX_{lag}'\n",
    "            index_full_date['temp'] = index_full_date['Closing Index Value'].rolling(lag).mean()\n",
    "            index_full_date[grad_col] = np.gradient(index_full_date['temp'], np.arange(index_full_date['temp'].shape[0]))\n",
    "\n",
    "        index_full_date.drop(['Index Name','temp'],axis=1,inplace=True)\n",
    "\n",
    "        eq_symbol  = pd.merge(eq_symbol_in, index_full_date, on='DATE', how=\"left\")\n",
    "\n",
    "    else:\n",
    "        eq_symbol = eq_symbol_in.copy()\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    pred_cols = eq_symbol.columns[eq_symbol.columns.str.contains('MA_')|eq_symbol.columns.str.contains('CROSSOVER')|eq_symbol.columns.str.contains('GRAD')]\n",
    "    eq_symbol_f=eq_symbol[list(chain.from_iterable([['DATE','DAY', 'CLOSE_PRICE'], target_col,list(pred_cols)]))].dropna()\n",
    "    ignore_ma = ['MA_7', 'MA_14','MA_30', 'MA_45', 'MA_60', 'MA_90', 'MA_180']\n",
    "    pred_cols = list(chain.from_iterable([[x for x in pred_cols if x not in ignore_ma], ['DAY']]))\n",
    "    \n",
    "    #--------------------------------Drop Strongly correlated variables------------------------------------------------------\n",
    "\n",
    "    if corr_drop:\n",
    "        pred_cols_f = [x for x in pred_cols if x not in drop_corr_vars(df = eq_symbol_f, cols = pred_cols, cutoff=0.8)]\n",
    "    else:\n",
    "        pred_cols_f = pred_cols.copy()\n",
    "    \n",
    "    eq_symbol_last6=eq_symbol[eq_symbol['DATE']>=(datetime.today()-timedelta(days=180))][list(chain.from_iterable([['SYMBOL', 'DATE', 'CLOSE_PRICE']\n",
    "                                                                                                                   , target_col,list(pred_cols),ignore_ma]))].copy()\n",
    "\n",
    "    #---------------------------Prepare for the model------------------------------------\n",
    "    for k, target in enumerate(target_col):\n",
    "        train_mat_x, test_mat_x, train_mat_y, test_mat_y = train_test_split(eq_symbol_f[pred_cols_f], eq_symbol_f[target], test_size=0.3, random_state=42)\n",
    "\n",
    "        dtrain = xgb.DMatrix(train_mat_x, label=train_mat_y.values.reshape(1,-1))\n",
    "        dtest = xgb.DMatrix(test_mat_x, label=test_mat_y.values.reshape(1,-1))\n",
    "        data_all = xgb.DMatrix(eq_symbol_last6[pred_cols_f])\n",
    "        \n",
    "        params = { 'objective': 'binary:logistic','eval_metric': 'auc', 'gamma':1, 'max_depth':5\n",
    "                                          , 'reg_lambda':1,'reg_alpha':1,  'learning_rate':0.1, 'min_child_weight':10}\n",
    "        # Train the classifier\n",
    "        model = xgb.train(params, dtrain,100)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        test_pred = model.predict(dtest)\n",
    "        train_pred = model.predict(dtrain)\n",
    "        all_pred = model.predict(data_all)\n",
    "        \n",
    "        # Compute the AUC\n",
    "        auc_test = roc_auc_score(test_mat_y, test_pred)\n",
    "        auc_train = roc_auc_score(train_mat_y, train_pred)\n",
    "        row={'SYMBOL':SYMBOL,'AUC_TRAIN':auc_train,'AUC_TEST':auc_test}\n",
    "\n",
    "        model_perf = best_cutoff_acc(train_actual = train_mat_y, train_prediction=train_pred, test_actual = test_mat_y, test_prediction = test_pred)\n",
    "\n",
    "        model_perf['SYMBOL']=SYMBOL\n",
    "        model_perf['AUC_TRAIN']=np.round(100*auc_train,1)\n",
    "        model_perf['AUC_test']=np.round(100*auc_test,1)\n",
    "    \n",
    "        target_df[target].loc[i] = [model_perf['SYMBOL'], model_perf['Best_Cutoff'], model_perf['Rank_Cutoff'], model_perf['Train_Accuracy']\n",
    "                           , model_perf['Test_Accuracy'], model_perf['AUC_TRAIN'], model_perf['AUC_test'] ]\n",
    "        eq_symbol_last6[target_suffix[target][0]] = all_pred\n",
    "        eq_symbol_last6[target_suffix[target][1]] = np.where(all_pred>=model_perf['Best_Cutoff'], 1,0)\n",
    "\n",
    "        \n",
    "    if (i==0):\n",
    "        pred_last6 = eq_symbol_last6.copy()\n",
    "    else:\n",
    "        pred_last6 = pd.concat([pred_last6, eq_symbol_last6],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4a0bc-ca3a-456d-b1d7-ec459a46be86",
   "metadata": {},
   "source": [
    "### Transform and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d33bf-a91e-49dd-9063-498b88ed5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['SYMBOL', 'DATE', 'CLOSE_PRICE', 'Prob_14', 'Prob_30', 'Predicted_14', 'Predicted_30'\n",
    "             , 'CROSSOVER_14', 'CROSSOVER_30', 'CROSSOVER_45', 'CROSSOVER_60', 'CROSSOVER_90', 'CROSSOVER_180']\n",
    "\n",
    "final_col_order = ['SYMBOL', 'COMPANY NAME', 'INDUSTRY', 'MCAP_INLAKHS', 'DATE', 'CLOSE_PRICE', 'Top_Rank', 'Rank', 'Score'\n",
    "                   , 'Prob_14', 'Prob_30', 'Predicted_14', 'Predicted_30', 'Test_Accuracy_14', 'Test_Accuracy_30'\n",
    "                   , 'Pred14_0_TGT_0', 'Pred14_0_TGT_1', 'Pred14_1_TGT_0', 'Pred14_1_TGT_1', 'Pred14_0_Acc', 'Pred14_1_Acc'\n",
    "                   , 'Target14_0_Sum', 'Target14_1_Sum', 'Pred30_0_TGT_0', 'Pred30_0_TGT_1', 'Pred30_1_TGT_0', 'Pred30_1_TGT_1'\n",
    "                   , 'Pred30_0_Acc', 'Pred30_1_Acc', 'Target30_0_Sum', 'Target30_1_Sum'\n",
    "                   , 'AUC_test_14', 'AUC_test_30' , 'Best_Cutoff_14', 'Rank_Cutoff_14', 'Best_Cutoff_30', 'Rank_Cutoff_30'\n",
    "                   , 'CROSSOVER_14', 'CROSSOVER_30', 'CROSSOVER_45', 'CROSSOVER_60', 'CROSSOVER_90', 'CROSSOVER_180'\n",
    "                   , 'Train_Accuracy_14','Train_Accuracy_30', 'AUC_TRAIN_14','AUC_TRAIN_30']\n",
    "\n",
    "sec_mcap = pd.read_csv(\"E:/Equity Research/Misc Files/Eq_Mcap_Industry_data.csv\")\n",
    "# Get model metrics for 30 days target\n",
    "perf_last6_30 = pred_last6.pivot_table(index='SYMBOL', columns = ['Predicted_30', 'TARGET_30'], aggfunc = 'count', values = 'CLOSE_PRICE', fill_value=0).reset_index()\n",
    "perf_last6_30.columns = ['SYMBOL', 'Pred30_0_TGT_0', 'Pred30_0_TGT_1', 'Pred30_1_TGT_0', 'Pred30_1_TGT_1']\n",
    "perf_last6_30['Pred30_0_Acc'] = np.round(100*perf_last6_30['Pred30_0_TGT_0']/(perf_last6_30['Pred30_0_TGT_0']+perf_last6_30['Pred30_0_TGT_1']),1)\n",
    "perf_last6_30['Pred30_1_Acc'] = np.round(100*perf_last6_30['Pred30_1_TGT_1']/(perf_last6_30['Pred30_1_TGT_0']+perf_last6_30['Pred30_1_TGT_1']),1)\n",
    "perf_last6_30['Target30_0_Sum'] = perf_last6_30['Pred30_0_TGT_0']+perf_last6_30['Pred30_1_TGT_0']\n",
    "perf_last6_30['Target30_1_Sum'] = perf_last6_30['Pred30_0_TGT_1']+perf_last6_30['Pred30_1_TGT_1']\n",
    "\n",
    "# Get model metrics for 14 days target\n",
    "perf_last6_14 = pred_last6.pivot_table(index='SYMBOL', columns = ['Predicted_14', 'TARGET_14'], aggfunc = 'count', values = 'CLOSE_PRICE', fill_value=0).reset_index()\n",
    "perf_last6_14.columns = ['SYMBOL', 'Pred14_0_TGT_0', 'Pred14_0_TGT_1', 'Pred14_1_TGT_0', 'Pred14_1_TGT_1']\n",
    "perf_last6_14['Pred14_0_Acc'] = np.round(100*perf_last6_14['Pred14_0_TGT_0']/(perf_last6_14['Pred14_0_TGT_0']+perf_last6_14['Pred14_0_TGT_1']),1)\n",
    "perf_last6_14['Pred14_1_Acc'] = np.round(100*perf_last6_14['Pred14_1_TGT_1']/(perf_last6_14['Pred14_1_TGT_0']+perf_last6_14['Pred14_1_TGT_1']),1)\n",
    "perf_last6_14['Target14_0_Sum'] = perf_last6_14['Pred14_0_TGT_0']+perf_last6_14['Pred14_1_TGT_0']\n",
    "perf_last6_14['Target14_1_Sum'] = perf_last6_14['Pred14_0_TGT_1']+perf_last6_14['Pred14_1_TGT_1']\n",
    "\n",
    "pred_latest_df = pred_last6[pred_last6['DATE']==pred_last6['DATE'].max()][keep_cols].copy()\n",
    "\n",
    "pred_latest_df = pd.merge(sec_mcap, pred_latest_df, on='SYMBOL')\n",
    "\n",
    "model_perf_df = pd.merge(perf_agg_14, perf_agg_30, on='SYMBOL')\n",
    "\n",
    "model_perf_df = pd.merge(pred_latest_df, model_perf_df, on =\"SYMBOL\")\n",
    "\n",
    "model_perf_df = pd.merge(model_perf_df, perf_last6_14, on =\"SYMBOL\")\n",
    "\n",
    "model_perf_df = pd.merge(model_perf_df, perf_last6_30, on =\"SYMBOL\")\n",
    "\n",
    "model_perf_df['Pred14_1_Acc'] = model_perf_df['Pred14_1_Acc'].fillna(0)\n",
    "model_perf_df['Pred30_1_Acc'] = model_perf_df['Pred30_1_Acc'].fillna(0)\n",
    "\n",
    "C1 = 100*(model_perf_df['Prob_14']*model_perf_df['Predicted_14']+model_perf_df['Prob_30']*model_perf_df['Predicted_30']) # Joint Probability\n",
    "C2 = (0.5*model_perf_df['Test_Accuracy_14'] + 0.5*model_perf_df['Pred14_1_Acc']) # Recent vs Overall Test Accuracy\n",
    "C3 = (0.5*model_perf_df['Test_Accuracy_30'] + 0.5*model_perf_df['Pred30_1_Acc']) # Recent vs Overall Test Accuracy\n",
    "C4 = 100*(0.2*model_perf_df['CROSSOVER_14']+ 0.2*model_perf_df['CROSSOVER_30'] + 0.2*model_perf_df['CROSSOVER_45'] + 0.2*model_perf_df['CROSSOVER_60'] + 0.1*model_perf_df['CROSSOVER_90'] + 0.1*model_perf_df['CROSSOVER_180'])\n",
    "\n",
    "model_perf_df['Score'] = C1+C2+C3+C4\n",
    "model_perf_df['Rank'] = model_perf_df['Score'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "model_perf_df['Top_Rank'] = np.where(model_perf_df['Rank']<=10, model_perf_df['Rank'].astype(str), \"10+\")\n",
    "\n",
    "model_perf_df = model_perf_df[final_col_order]\n",
    "\n",
    "file_date = pred_last6['DATE'].max().date()\n",
    "\n",
    "pred_last6['Pred_Acc_14'] = np.where(pred_last6['TARGET_14'].isna(), np.NaN\n",
    "                                     , np.where(pred_last6['TARGET_14']==pred_last6['Predicted_14'],1,0))\n",
    "\n",
    "pred_last6['Predicted_14'] = np.where(pred_last6['TARGET_14'].isna(), pred_last6['Predicted_14'], np.NaN )\n",
    "\n",
    "\n",
    "pred_last6['Pred_Acc_30'] = np.where(pred_last6['TARGET_30'].isna(), np.NaN\n",
    "                                     , np.where(pred_last6['TARGET_30']==pred_last6['Predicted_30'],1,0))\n",
    "\n",
    "pred_last6['Predicted_30'] = np.where(pred_last6['TARGET_30'].isna(), pred_last6['Predicted_30'], np.NaN )\n",
    "\n",
    "pred_last6['Yr_Month'] = [f'{x[0:3]},{y-2000}' for x,y in zip(pred_last6['DATE'].dt.month_name().tolist(), pred_last6['DATE'].dt.year.tolist())]\n",
    "\n",
    "# Add the rank to pred last 6\n",
    "pred_last6 = pd.merge(model_perf_df[['SYMBOL', 'Top_Rank', 'Rank']], pred_last6, on='SYMBOL')\n",
    "\n",
    "model_perf_df.to_csv(f\"E:/Equity Research/Model_Predictions/model_perf_df_{file_date}.csv\", index=False)\n",
    "pred_last6.to_csv(f\"E:/Equity Research/Model_Predictions/pred_last6_{file_date}.csv\", index=False)\n",
    "\n",
    "model_perf_df.to_csv(f\"E:/Equity Research/Model_Predictions/model_perf_df.csv\", index=False)\n",
    "pred_last6.to_csv(f\"E:/Equity Research/Model_Predictions/pred_last6.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea9956-ed0a-40d4-a33f-dc59fc303323",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4fd51-9ada-40c6-9ae5-7d4a948d8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df[['Predicted_14', 'Predicted_30']].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62581ae6-9ca3-4c59-a4e3-60e9a4937935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df[[ 'Test_Accuracy_14', 'Test_Accuracy_30', 'AUC_test_14', 'AUC_test_30']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c52db-4e49-434a-9101-bc47a4b78d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df[[ 'Train_Accuracy_14', 'Train_Accuracy_30', 'AUC_TRAIN_14', 'AUC_TRAIN_30']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acea3f-b65f-4c87-807b-388603361482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "model_perf_df[[ 'Best_Cutoff_14', 'Rank_Cutoff_14', 'Best_Cutoff_30', 'Rank_Cutoff_30']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d380d6c-799d-459c-9f19-05365d284d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# model_perf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8a0d0-8daa-4bdc-8319-c6b1d89ea151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
